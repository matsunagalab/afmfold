\subsection{\Model computational protocol}
\label{subsec:afm-guidance}

%Compared with previous simulation-based approaches, \Model offers a significant advantage of computational cost. Specifically, by relying on the diffusion process of \AFiii, which exploits co-evolutionary information, it is possible to generate conformations consistent with AFM images at substantially reduced computational cost. 

%On the other hand, this approach transforms AFM images into inter-domain distance via the CNN, that are easy to compute even for the structure in the generation process of \AFiii, rather than uses the raw images directly. During this transformation, the pixel-level height information is compressed into low-dimensional inter-domain distances. As a result, the rich structural information inherently present in AFM images cannot be fully utilized, which imposes limitations on prediction accuracy. Therefore, improving the accuracy of \Model requires improving the prediction accuracy of the CNN itself, to preserve as much structural information as possible during this compression.

%In this section, we introduce the concrete procedure of \Model, with a particular focus on the CNN training strategy, which plays a critical role in determining the overall accuracy of the method.

Our \Model framework consists of two main steps as described in the previous subsections: (i) estimation of CVs from AFM images using a \gcnn, and (ii) navigating the generative process of \AFiii using the estimated CVs. However, additional steps are still required for preparing training data for the \gcnn, training, inference, and evaluation. In this subsection, we describe these steps.

\subsubsection{Preparation of training data}
\label{subsubsec:afm-prep}

To train the \gcnn to learn the relationship between AFM images and underlying 3D structures, we construct training data by generating pseudo-AFM images from 3D conformations and pairing them with ground-truth CV labels. In this study, these conformations are generated by applying diverse CV-based restraints to \AFiii. This strategy offers two key advantages: (i) first, compared to other computational tools to generate 3D structures (such as MD simulations), the required computational resources are substantially reduced; (ii) second, this approach allows for sufficient sampling of intermediate conformations so that the \gcnn can learn to interpolate the structures in the conformational space. In order to detect smooth conformational transitions, the \gcnn requires training data not only for two boundary conformations (e.g., start and end conformations) but also for intermediate conformations continuously linking them. In the absence of such intermediates, the \gcnn is unable to accurately model continuous structural variability. By specifying the CV values continuously, we can generate diverse conformations that cover the conformational space.

\paragraph{Generating candidate conformations to broadly cover the CV space}
We sampled diverse conformations by \AFiii, broadly covering the CV space specifying the CV values with grids. 
%large-scale domain-level rearrangements underlying the AFM observations. 
%If the coverage is too narrow, the CNN may fail to learn a stable AFM$\rightarrow \phi$ mapping for unseen configurations. 
%In this sense, this exploration corresponds to formulating hypotheses for predicting inter-domain distances from AFM images.
%The target inter-domain distance restraints for \AFiii are composed as below.
To do this, first, we obtain a reference structure $X_{\text{ref}}$ with \AFiii with no restraints.
Then, target CV vectors $\{\phi_{\text{perturb}}^{\,i}\}$ are placed on a grid around $\phi(X_{\text{ref}})$ such that, for each dimension $i$,
\begin{equation}
    \phi_{\text{perturb},d}^i \in \left[ \min(0.0~\mathrm{nm}, \phi^i(X_{\text{ref}}) - 5.0~\mathrm{nm}), \phi^i(X_{\text{ref}}) + 5.0~\mathrm{nm} \right], \qquad \forall i.
    \label{eq:target_domain_distance}
\end{equation}
The grid used a step of $0.5~\mathrm{nm}$ for each axis. Then, the generation with \AFiii is conducted using each $\{\phi_{\text{perturb}}^{\,i}\}$ as the restraints for the navigating process.

%\paragraph{Early rejection criteria and stopping strategy}
%In the generation step just before, although candidate conformations can span a wide range of $\phi$-space, the number of grid points grows rapidly with the dimensionality (i.e., the number of chosen domain pairs). 
%While diversity is important, many unphysical conformations will be filtered out later during sanitization, so it is preferable to avoid generating such conformations in the first place. 

%We therefore defined a conformation as \emph{invalid} if fewer than $92\%$ of its C$\alpha$â€“C$\alpha$ bond lengths fall within $[0.37,0.39]~\mathrm{nm}$. 
%Conformations are generated with restraint targets sorted by increasing distance from $\phi(X_{\text{ref}})$, starting from $\phi_{\text{perturb}}=\phi(X_{\text{ref}})$. 
%The exploration is terminated once all previously generated candidates within a three-step neighborhood on the grid (i.e., nodes reachable within at most three edges) are invalid.

\paragraph{Geometric sanitization}
The candidate conformations generated in the previous step may contain geometric violations (e.g., steric clashes, backbone/side-chain outliers). 
Such conformations could bias the \gcnn by introducing unphysical patterns without valid conformational counterparts. 
To exclude such data from the training set, 
we applied thresholds to perform geometric sanitization, based on MolProbity scores (\Cref{tab:score_thresholds}), excluding clear violations. 
This procedure constrains the \gcnn to perform inference within a conformational space devoid of geometric violations. The resulting set that passes this sanitization serves as 
a training set for training the \gcnn and for synthesizing pseudo-AFM images used in training.

\begin{table}[H]
\centering
\caption{MolProbity score thresholds used for geometric sanitization.}
\label{tab:score_thresholds}
\begin{tabular}{|l|c|}
\hline
Criterion & Threshold \\
\hline
MolProbity Score & $\leq 4.0$ \\
Clash Score & $\leq 70.0$ \\
Ramachandran favored (\%) & $\geq 90.0$ \\
Rotamer Outlier (\%) & $\leq 50.0$ \\
\hline
\end{tabular}
\end{table}

\paragraph{Pseudo-AFM image rendering.}

In rendering the pseudo-AFM, we used the morphology functions implemented in our previous work \cite{matsunaga2023endtoend}. The specific parameter settings are summarized in \Cref{tab:afm_settings}. It should be noted that for the test case of \AK, we used smaller values for the tip radius and finer resolution of pseudo-AFM images compared to typical experimental conditions. On the other hand, for \flhac, since the precise tip geometry is unknown in experiments, the probe radius $r$ was sampled uniformly within a range. Moreover, the height distribution of pseudo-AFM images was fitted to the real experimental images using \verb|skimage.exposure.match_histograms|. For both \AK and \flhac, we used the inter-domain distances (distances between the centers of mass of C$\alpha$ atoms in the domains) as the CVs.

\begin{table}[htbp]
\centering
\caption{Settings for training pseudo-AFM images.}
\label{tab:afm_settings}
\begin{tabular}{|l|c|c|}
\hline
 & \AK & \flhac \\
\hline
Lateral resolution [nm/pixel] & 0.3 & 0.98 \\
Pixel size [pixel $\times$ pixel] & $35 \times 35$ & $35 \times 35$ \\
Tip radius [nm] & $r \sim \mathrm{Uniform}(0.3,0.6)$ & $r \sim \mathrm{Uniform}(2.0,6.0)$ \\
Tip apex angle [degree] & $a \sim \mathrm{Uniform}(10,30)$ & $a \sim \mathrm{Uniform}(10,30)$ \\
Noise std. dev. [nm] & 0.0 & 0.5 \\
Histogram matching & --- & \checkmark \\
Dataset size [frames] & 5M & 5M \\
Collective variables & Inter-domain distances & Inter-domain distances \\
                     & (LID-CORE, CORE-AMPbd, & (\acdi-\acdiv, \acdii-\acdiii, \\
                     & AMPbd-LID)  & \acdii-\acdiv)  \\
\hline
\end{tabular}
\end{table}

\subsubsection{Training}
\label{subsubsec:afm-training}

Here, the training of the g-CNN can be regarded as supervised regression, where the task is to predict the CV values from a given AFM image. 
As the most straightforward loss function for supervised regression, we adopted the mean squared error (MSE) loss in the CV space:
\begin{equation}
    \mathcal{L}_{\text{CNN}} = \mathrm{mean}_{i} \| \phi_{\text{pred}}(I_i) - \phi_{\text{true}}(I_i) \|^2,
    \label{eq:cnn-mse-loss}
\end{equation}
where $I_i$ denotes the $i$-th input pseudo-AFM image, $\phi_{\text{pred}}(I_i)$ is the CV values predicted by the g-CNN, and $\phi_{\text{true}}(I_i)$ is the corresponding ground-truth CV values. As the g-CNN architecture, we implemented a $(\mathbb{R}^2, +) \rtimes C_8$-invariant CNN (see SI section S2 for the implementation details).

All models in this paper were trained on a single node equipped with an NVIDIA RTX A6000 GPU (48~GB memory). The training took approximately 2--3 days for both \AK and \flhac. 

\subsubsection{Inference}
\label{subsubsec:afm-inference}

%An important aspect in reconstructing conformations from AFM images is that the data are inherently time series. By analyzing AFM images as a temporal sequence, one can visualize domain fluctuations and morphing pathways of proteins. This perspective allows them to be interpreted as parts of continuous transitions, and provides the opportunity to establish connections with theoretical frameworks such as molecular simulations or Markov state models. Accordingly, computational efficiency that enables the rapid processing of multiple frames is essential.

The inference step consists of the estimation of CVs (inter-domain distances) from the AFM image by the g-CNN, and the navigation of the generative process of \AFiii using the estimated CVs. 
In this study, we require that the MSE of the g-CNN for the generated structures be below about $3~\mathrm{nm}$, 
which is an important resolution for distinguishing different conformations for both \AK and \flhac. 
%As will be shown in \Cref{sec:results}, this requirement was met for both systems. 

The computation time for the inference step was extremely short 
compared to flexible fitting calculations using MD simulations. 
Indeed, inference time was less than one minute per image for both \AK and \flhac.

%Notably, for both \AK and \flhac, the inference was completed in less than one minute.

%In this study, because the guidance scheduling $\eta_t$ was adjusted, the inter-domain distances of generated 
%conformations often deviated from the restraints. To address this issue, instead of directly adopting the CNN's output as $\phi_{\text{target}}$, we set restrictions based on past predictions, typically the conformation pool created during training data generation. 

\subsubsection{Evaluation}
\label{subsubsec:afm-evaluation}

To evaluate the accuracy of the estimated conformation, we computed the root mean square deviation (RMSD) between the estimated and the ground-truth conformations. 
Also, to evaluate how well the estimated conformation reproduces the given AFM image, we performed a rigid-body fitting, optimizing 3D rotation, translation, and tip radius.
The optimization is evaluated through the correlation coefficient (c.c.) with the given AFM image, using \Cref{eq:cc} \cite{Niina2020}.  
The pose achieving the maximum c.c. value across all rotations, translations and tip radii 
was taken as the optimal pose. 
\begin{equation}
    \mathrm{c.c.}(R)=
    \frac{\sum_{p \in \text{pixels}} H^{\text{(exp)}}_p H^{\text{(sim)}}_p(R)}
         {\sqrt{\sum_{p \in \text{pixels}} \left(H^{\text{(exp)}}_p\right)^2}
          \sqrt{\sum_{p \in \text{pixels}} \left(H^{\text{(sim)}}_p(R)\right)^2}}
    \label{eq:cc}
\end{equation}
where $H^{\text{(exp)}}_p$ and $H^{\text{(sim)}}_p(R)$ denote the heights (i.e., the $z$-coordinates) of the given and generated image pixels (transformed by $R$), respectively, and the summation runs over all pixels $p$.
