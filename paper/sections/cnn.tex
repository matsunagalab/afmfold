\subsection{Group-equivariant CNNs}
\label{subsec:se2cnn}

% --- Setup/context ---
%Unlike many other problems, reconstructing 3D conformations from AFM images requires verifying whether the 3D structures generated by \AFiii are consistent with a given 2D reference image. The generation process of \AFiii is guided to maximize the resulting likelihood, thereby producing conformations that agree closely with AFM images (as explained in \cref{subsec:guided-diffusion}). However, because the orientation of the molecule in the reference image is unknown and may correspond to any element of the full 3D rotation group, a naive search for the ground-truth pose is computationally prohibitive. Thus, an effective strategy is needed to assess the agreement between 2D AFM images and 3D molecular conformations. In particular, rather than transforming conformations into the AFM image format—--which is computationally demanding due to the rotational degrees of freedom—--it is more practical to derive features from AFM images that are more readily accessible from conformational side. 

In AFM 2D image analysis, similar to single particle analysis of cryoEM micrographs, the pose of the 3D structure is unknown. The alignment calculation between a 2D  image and the 3D structural pose requires exhaustive search and is computationally expensive. Moreover, if misalignment occurs, it leads to extreme deterioration in estimation accuracy. In the case of cryoEM, the influence of misalignment can be mitigated on average by a vast number of images, but in the case of AFM, since we want to perform structural estimation for each single image, misalignment cannot be tolerated. Therefore, we decided to use a rotation-equivariant CNN model (\emph{group-equivariant} CNN; g-CNN) \cite{CohenWelling2016_GCNN, CohenWelling2017_SteerableCNNs, e2cnn, WeilerEtAl2018_3DSteerableCNNs} that directly estimates structure-related CVs from 2D AFM images without performing alignment calculations. 

In steering the structure generation process of \AFiii to generate 3D structures consistent with AFM images, it would be possible to optimize by calculating image similarity each time, as done in rigid-body fitting. In this case, higher image reproduction is expected, but on the other hand, we empirically found that not only does computational cost increase, but overfitting to images causes rotamer outliers and atomic clashes, compromising the physical validity of the structure. Therefore, in this study, we decided that our g-CNN model estimates low-dimensional CVs (such as inter-domain distances in the case of multi-domain proteins) and uses them for structure generation in \AFiii.

With this motivation in mind, we summarize the theory of g-CNN (\gcnn) \cite{CohenWelling2016_GCNN, CohenWelling2017_SteerableCNNs, e2cnn, WeilerEtAl2018_3DSteerableCNNs} based approach that enables efficient comparison between AFM images and conformations. A standard CNN is naturally \emph{translation equivariant} but not rotation equivariant. In contrast, \gcnn modifies feature indexing and kernel weight sharing so that the entire network becomes equivariant with respect to a chosen rotation group. With this architecture, the same physical state yields an identical representation regardless of its placement on the image plane, thereby reducing the need for extensive data augmentation and shortening training time. Consequently, we employ the \gcnn to estimate CVs from a single AFM image.

%\paragraph{Translation equivariance}
We model an AFM image as a single-channel (scalar) field $I:\mathbb{R}^2\to\mathbb{R}$ with pixel coordinate $x=(x_1,x_2)\in\mathbb{R}^2$. 
In a standard CNN, intermediate feature maps are represented as functions $f:\mathbb{R}^2 \to \mathbb{R}^c$.
The input image is first mapped to a $c$-channel field by convolution with channel-specific filters $\{\psi_i\}_{i=1}^c$:
\begin{equation}
    (\psi * I)(x)[i] := \int_{\mathbb{R}^2} \psi_i(y)\, I(x - y)\, dy,
    \label{eq:lifting}
\end{equation}
where $*$ is convolution and $dy$ denotes the area element. 
In practice, the integral is implemented as a finite sum over pixels; we keep the continuous notation for clarity.

% --- Group and action for standard fields ---
Here, we consider the planar motion group $\R^2 \rtimes C_N$, where $C_N=\{e,r,\dots,r^{N-1}\}\subset \mathrm{SO}(2)$ is a finite rotation subgroup.
For scalar or standard $c$-channel fields (without group indexing that we later explain), 
the action $\pi(t,g)$ of translation $t\in\R^2$ and rotation $g\in C_N$ is $(\pi(t,g)u)(x)\;:=\;u\big(g^{-1}(x-t)\big)$.

Because convolution commutes with translations, a standard CNN is translation-equivariant. 
For a single convolution with a (matrix-valued) kernel $k(y)\in\R^{c_{\mathrm{out}}\times c_{\mathrm{in}}}$,
\begin{equation}
    \begin{aligned}
        \big(k * (\pi(t,e) u)\big)(x) 
        &= \int_{\mathbb{R}^2} k(y)\, u(x-y-t)\, dy \\
        &= (k * u)(x - t) = \big(\pi(t,e) (k * u)\big)(x).
    \end{aligned}
    \label{eq:trans-equiv}
\end{equation}
Since a typical activation $\sigma$ is pointwise, $\sigma$ is also translation-equivariant, $\sigma (\pi(t, e) u) = \pi(t, e) \sigma (u)$ and the property propagates through the network. By contrast, the standard convolution is generally \emph{not} rotation-equivariant.

%\paragraph{Rotation equivariance}
The key idea of \gcnns is to \emph{lift} an image to a feature field indexed by group elements and to impose weight sharing consistent with the group action.
Given a base filter $\psi:\mathbb{R}^2\to\mathbb{R}$, define its rotated copies $\psi_h(y):=\psi(h^{-1}y)$ for $h\in C_N$, and set
\begin{equation}
    v(x)[h] \;:=\; \int_{\mathbb{R}^2} \psi(h^{-1}y)\, I(x-y)\, dy,
    \qquad h\in C_N.
    \label{eq:gcnn-lifting}
\end{equation}
Thus $v:\mathbb{R}^2\times C_N\to\mathbb{R}$ is a group-indexed (orientation-channel) field.

% --- Equivariance statement (concise) ---
For such lifted fields, if the kernel satisties the following constraint about sharing weights, $k(gy)[h,h'] = k(y) \big[g^{-1}h, g^{-1}h'\big]$ for all $g, h, h' \in C_N$, then the group convolution commutes with $\pi(0,g)$:
\begin{equation}
    \big(k * (\pi(0,g)v)\big)(x)[h] \;=\; \big(\pi(0,g)(k * v)\big)(x)[h].
    \label{eq:gcnn-rot-equiv}
\end{equation}
For details, see \cite{e2cnn}. 
Hence, adding to the translation equivariance \Cref{eq:trans-equiv}, we can construct CNNs equivariant to the chosen discrete rotation group $C_N$. 
Channel-wise activations commute with the orientation relabeling, so equivariance to rotations (and translations) is preserved layer by layer.

% --- Multi-block and pooling ---
A practical architecture uses multiple lifted blocks $v_1,\dots,v_b$; the overall action is the block-diagonal (direct-sum) representation $\pi=\bigoplus_{i=1}^b \pi_i$. In \Model, because we require a vector invariant to rotations and translations, we apply a group pooling and a spacial pooling per block at last; i.e.,
\begin{equation}
    \mathrm{Pool}(v_i)(x) := \max_{h\in C_N} v_i(x)[h], \quad
    \mathrm{Pool}(v_i) := \text{Mean}_{x \in \R^2} v_i(x),
\end{equation}
which yields features invariant to translations and rotations.
